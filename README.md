# üìö Sistema de An√°lisis Bibliom√©trico y Algoritmos de Ordenamiento

**Proyecto Acad√©mico de An√°lisis Bibliom√©trico Automatizado**

[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org/)
[![License](https://img.shields.io/badge/License-Academic-green.svg)]()
[![Status](https://img.shields.io/badge/Status-Active-success.svg)]()

---

## üìã Tabla de Contenidos

1. [Descripci√≥n General](#-descripci√≥n-general)
2. [Arquitectura del Sistema](#-arquitectura-del-sistema)
3. [Estructura del Proyecto](#-estructura-del-proyecto)
4. [Requisitos del Sistema](#-requisitos-del-sistema)
5. [Instalaci√≥n](#-instalaci√≥n)
6. [Requerimientos Funcionales](#-requerimientos-funcionales)
7. [Uso del Sistema](#-uso-del-sistema)
8. [Interfaz Web](#-interfaz-web)
9. [Metodolog√≠a Cient√≠fica](#-metodolog√≠a-cient√≠fica)
10. [Resultados y Outputs](#-resultados-y-outputs)
11. [Troubleshooting](#-troubleshooting)
12. [Contribuciones](#-contribuciones)

---

## üéØ Descripci√≥n General

Este proyecto implementa un **sistema completo de an√°lisis bibliom√©trico** que combina:

1. **Web Scraping Automatizado**: Extracci√≥n de art√≠culos acad√©micos desde bases de datos cient√≠ficas (ACM Digital Library, SAGE Journals)
2. **An√°lisis de Algoritmos de Ordenamiento**: Implementaci√≥n y comparaci√≥n de 12 algoritmos de ordenamiento sobre datos bibliogr√°ficos
3. **An√°lisis de Similitud Textual**: Comparaci√≥n de abstracts usando t√©cnicas NLP cl√°sicas y modelos de IA
4. **An√°lisis de Frecuencia**: Extracci√≥n autom√°tica de t√©rminos relevantes usando TF-IDF
5. **Clustering Jer√°rquico**: Agrupamiento de documentos por similitud sem√°ntica
6. **Visualizaciones Avanzadas**: Mapas de calor geogr√°ficos, nubes de palabras, l√≠neas temporales

### üéì Contexto Acad√©mico

El proyecto fue dise√±ado para satisfacer los requisitos de un curso de **Estructuras de Datos y Algoritmos**, demostrando:
- Implementaci√≥n pr√°ctica de algoritmos de ordenamiento
- An√°lisis de complejidad temporal y espacial
- Aplicaci√≥n de estructuras de datos (√°rboles, heaps, buckets)
- Procesamiento de grandes vol√∫menes de datos bibliogr√°ficos
- Visualizaci√≥n de resultados cient√≠ficos

---

## üèóÔ∏è Arquitectura del Sistema

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    INTERFAZ DE USUARIO                      ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îÇ
‚îÇ  ‚îÇ   CLI (main.py) ‚îÇ         ‚îÇ  Web UI (webui.py)     ‚îÇ     ‚îÇ
‚îÇ  ‚îÇ   run_all.py    ‚îÇ         ‚îÇ  Flask Server          ‚îÇ     ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                      CAPA DE L√ìGICA                           ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îÇ
‚îÇ  ‚îÇ  Requirement ‚îÇ  ‚îÇ  Requirement ‚îÇ  ‚îÇ  Requirement ‚îÇ         ‚îÇ
‚îÇ  ‚îÇ      1       ‚îÇ  ‚îÇ      2       ‚îÇ  ‚îÇ      3       ‚îÇ         ‚îÇ
‚îÇ  ‚îÇ  (Scraping)  ‚îÇ  ‚îÇ (Similarity) ‚îÇ  ‚îÇ (Frequency)  ‚îÇ         ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îÇ
‚îÇ  ‚îÇ  Requirement ‚îÇ  ‚îÇ  Requirement ‚îÇ  ‚îÇ Seguimiento  ‚îÇ         ‚îÇ
‚îÇ  ‚îÇ      4       ‚îÇ  ‚îÇ      5       ‚îÇ  ‚îÇ      1       ‚îÇ         ‚îÇ
‚îÇ  ‚îÇ (Clustering) ‚îÇ  ‚îÇ(Visualizat.) ‚îÇ  ‚îÇ (Sorting)    ‚îÇ         ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                     CAPA DE DATOS                             ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê            ‚îÇ
‚îÇ  ‚îÇ  Raw Data  ‚îÇ  ‚îÇ  Processed  ‚îÇ  ‚îÇ   Outputs    ‚îÇ            ‚îÇ
‚îÇ  ‚îÇ  (BibTeX)  ‚îÇ  ‚îÇ    Data     ‚îÇ  ‚îÇ  (CSV/JSON)  ‚îÇ            ‚îÇ
‚îÇ  ‚îÇ  data/raw/ ‚îÇ  ‚îÇdata/process/‚îÇ  ‚îÇdata/process/ ‚îÇ            ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üìÅ Estructura del Proyecto

```
Bibliometria_AA/
‚îÇ
‚îú‚îÄ‚îÄ üìÑ main.py                    # Punto de entrada CLI principal
‚îú‚îÄ‚îÄ üìÑ run_all.py                 # Ejecutor secuencial de todos los requerimientos
‚îú‚îÄ‚îÄ üìÑ webui.py                   # Interfaz web Flask
‚îú‚îÄ‚îÄ üìÑ requirements.txt           # Dependencias Python
‚îú‚îÄ‚îÄ üìÑ .gitignore                 # Archivos excluidos de Git
‚îÇ
‚îú‚îÄ‚îÄ üìÅ requirement_1/             # REQUERIMIENTO 1: Web Scraping
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ scrapers/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ acm_scraper.py       # Scraper para ACM Digital Library
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ sage_scraper.py      # Scraper para SAGE Journals
‚îÇ   ‚îî‚îÄ‚îÄ unificar.py              # Unificador de archivos BibTeX
‚îÇ
‚îú‚îÄ‚îÄ üìÅ requirement_2/             # REQUERIMIENTO 2: Similitud Textual
‚îÇ   ‚îú‚îÄ‚îÄ run_similarity.py        # Ejecutor principal
‚îÇ   ‚îú‚îÄ‚îÄ classic.py               # Algoritmos cl√°sicos (TF-IDF, LSA, LDA)
‚îÇ   ‚îú‚îÄ‚îÄ ai_models.py             # Modelos embeddings (SentenceTransformer)
‚îÇ   ‚îú‚îÄ‚îÄ preprocessing.py         # Limpieza y normalizaci√≥n de texto
‚îÇ   ‚îú‚îÄ‚îÄ reports.py               # Generaci√≥n de reportes Markdown/CSV
‚îÇ   ‚îî‚îÄ‚îÄ console_report.py        # Resumen en consola
‚îÇ
‚îú‚îÄ‚îÄ üìÅ requirement_3/             # REQUERIMIENTO 3: Frecuencia y T√©rminos
‚îÇ   ‚îú‚îÄ‚îÄ run_req3.py              # Pipeline principal
‚îÇ   ‚îú‚îÄ‚îÄ frequency.py             # C√°lculo de frecuencias
‚îÇ   ‚îú‚îÄ‚îÄ auto_terms.py            # Extracci√≥n autom√°tica (TF-IDF)
‚îÇ   ‚îú‚îÄ‚îÄ evaluate.py              # Evaluaci√≥n de precisi√≥n
‚îÇ   ‚îú‚îÄ‚îÄ keywords.py              # T√©rminos semilla
‚îÇ   ‚îî‚îÄ‚îÄ data_loader.py           # Carga de datos BibTeX
‚îÇ
‚îú‚îÄ‚îÄ üìÅ requirement_4/             # REQUERIMIENTO 4: Clustering
‚îÇ   ‚îú‚îÄ‚îÄ run_req4.py              # Ejecutor principal
‚îÇ   ‚îú‚îÄ‚îÄ clustering.py            # Algoritmos jer√°rquicos
‚îÇ   ‚îî‚îÄ‚îÄ dendrograms.py           # Visualizaci√≥n de dendrogramas
‚îÇ
‚îú‚îÄ‚îÄ üìÅ requirement_5/             # REQUERIMIENTO 5: Visualizaciones
‚îÇ   ‚îú‚îÄ‚îÄ run_req5.py              # Pipeline visual completo
‚îÇ   ‚îú‚îÄ‚îÄ geo.py                   # Mapas de calor geogr√°ficos
‚îÇ   ‚îú‚îÄ‚îÄ wordcloud_gen.py         # Nubes de palabras
‚îÇ   ‚îú‚îÄ‚îÄ timeline.py              # L√≠neas temporales
‚îÇ   ‚îî‚îÄ‚îÄ data_loader5.py          # Cargador de datos
‚îÇ
‚îú‚îÄ‚îÄ üìÅ Seguimiento_1/             # SEGUIMIENTO 1: Algoritmos de Ordenamiento
‚îÇ   ‚îú‚îÄ‚îÄ algoritmos_ordenamiento.py    # Ejecutor de todos los algoritmos
‚îÇ   ‚îú‚îÄ‚îÄ author_range.py               # An√°lisis por rango de autores
‚îÇ   ‚îî‚îÄ‚îÄ stats_algoritmos.py           # Estad√≠sticas y gr√°ficos
‚îÇ
‚îú‚îÄ‚îÄ üìÅ data/                      # DIRECTORIO DE DATOS
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ raw/                   # Datos crudos descargados
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ acm/                 # BibTeX de ACM
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ sage/                # BibTeX de SAGE
‚îÇ   ‚îî‚îÄ‚îÄ üìÅ processed/             # Datos procesados
‚îÇ       ‚îú‚îÄ‚îÄ productos_unificados.bib    # BibTeX unificado
‚îÇ       ‚îú‚îÄ‚îÄ ordenamiento/               # Archivos ordenados
‚îÇ       ‚îî‚îÄ‚îÄ algoritmos_ordenamiento/    # Scripts de ordenamiento
‚îÇ           ‚îú‚îÄ‚îÄ timsort.py
‚îÇ           ‚îú‚îÄ‚îÄ quicksort.py
‚îÇ           ‚îú‚îÄ‚îÄ heap_sort.py
‚îÇ           ‚îú‚îÄ‚îÄ radix_sort.py
‚îÇ           ‚îú‚îÄ‚îÄ bucket_sort.py
‚îÇ           ‚îú‚îÄ‚îÄ comb_sort.py
‚îÇ           ‚îú‚îÄ‚îÄ binary_insertion.py
‚îÇ           ‚îú‚îÄ‚îÄ bitonic_sort.py
‚îÇ           ‚îú‚îÄ‚îÄ selection_sort.py
‚îÇ           ‚îú‚îÄ‚îÄ gnome_sort.py
‚îÇ           ‚îú‚îÄ‚îÄ pigeonhole.py
‚îÇ           ‚îî‚îÄ‚îÄ treesort.py
‚îÇ
‚îú‚îÄ‚îÄ üìÅ templates/                 # Plantillas HTML (Flask)
‚îÇ   ‚îî‚îÄ‚îÄ index.html               # Interfaz web principal
‚îÇ
‚îî‚îÄ‚îÄ üìÅ utils/                     # Utilidades compartidas
    ‚îî‚îÄ‚îÄ helpers.py               # Funciones auxiliares
```

---

## üíª Requisitos del Sistema

### Requisitos de Hardware

- **CPU**: Procesador multi-core (recomendado: 4+ cores)
- **RAM**: M√≠nimo 8 GB (recomendado: 16 GB para an√°lisis grandes)
- **Almacenamiento**: 2 GB de espacio libre
- **Internet**: Conexi√≥n estable para web scraping

### Requisitos de Software

```yaml
Sistema Operativo:
  - Windows 10/11
  - macOS 10.15+
  - Linux (Ubuntu 20.04+)

Python:
  - Versi√≥n: 3.8 o superior
  - pip: √öltima versi√≥n

Navegador (para scraping):
  - Google Chrome: √öltima versi√≥n
  - ChromeDriver: Compatible con Chrome
```

### Dependencias Python

```txt
# Web Scraping
selenium==4.15.0
undetected-chromedriver==3.5.4
webdriver-manager==4.0.1
beautifulsoup4==4.12.2
requests==2.31.0

# Procesamiento de Datos
pandas==2.1.3
numpy==1.24.3
bibtexparser==1.4.0

# NLP y Machine Learning
scikit-learn==1.3.2
sentence-transformers==2.2.2
transformers==4.35.2
torch==2.1.1
nltk==3.8.1
gensim==4.3.2

# Visualizaci√≥n
matplotlib==3.8.2
seaborn==0.13.0
plotly==5.18.0
wordcloud==1.9.3
kaleido==0.2.1

# Geograf√≠a
pycountry==23.12.11

# Web Framework
flask==3.0.0

# Utilidades
tqdm==4.66.1
openpyxl==3.1.2
```

---

## üöÄ Instalaci√≥n

### 1. Clonar el Repositorio

```bash
# Clonar desde GitHub
git clone https://github.com/nestorcastelblanco/Bibliometria_AA.git
cd Bibliometria_AA
```

### 2. Crear Entorno Virtual

#### En Windows:
```bash
python -m venv venv
venv\Scripts\activate
```

#### En macOS/Linux:
```bash
python3 -m venv venv
source venv/bin/activate
```

### 3. Instalar Dependencias

```bash
# Actualizar pip
python -m pip install --upgrade pip

# Instalar todas las dependencias
pip install -r requirements.txt

# Descargar recursos NLTK (requerido)
python -c "import nltk; nltk.download('punkt'); nltk.download('stopwords')"
```

### 4. Configurar ChromeDriver (para scraping)

El sistema usa `undetected-chromedriver` que gestiona ChromeDriver autom√°ticamente.
**No es necesario instalar ChromeDriver manualmente.**

Aseg√∫rate de tener Google Chrome instalado en tu sistema.

### 5. Verificar Instalaci√≥n

```bash
# Verificar versi√≥n de Python
python --version  # Debe ser 3.8+

# Verificar que todas las dependencias se instalaron
pip list

# Prueba r√°pida
python main.py --help
```

---

## üîß Requerimientos Funcionales

### üìä Requerimiento 1: Web Scraping Automatizado

**Objetivo**: Extraer art√≠culos acad√©micos de bases de datos cient√≠ficas.

#### Caracter√≠sticas:
- ‚úÖ Scraping de ACM Digital Library
- ‚úÖ Scraping de SAGE Journals
- ‚úÖ Navegaci√≥n autom√°tica por p√°ginas de resultados
- ‚úÖ Selecci√≥n masiva de art√≠culos
- ‚úÖ Exportaci√≥n a formato BibTeX
- ‚úÖ Manejo de sesiones autenticadas
- ‚úÖ Modo headless (sin interfaz gr√°fica)

#### Tecnolog√≠as:
- **Selenium** con `undetected-chromedriver` (evita detecci√≥n de bots)
- **BeautifulSoup4** para parsing HTML
- **Requests** para descargas HTTP

#### Outputs:
```
data/raw/acm/
  ‚îî‚îÄ‚îÄ articulos_acm_YYYYMMDD_HHMMSS.bib
data/raw/sage/
  ‚îî‚îÄ‚îÄ articulos_sage_YYYYMMDD_HHMMSS.bib
data/processed/
  ‚îî‚îÄ‚îÄ productos_unificados.bib  # Archivo consolidado
```

---

### üîÄ Seguimiento 1: Algoritmos de Ordenamiento

**Objetivo**: Implementar y comparar 12 algoritmos de ordenamiento sobre datos bibliogr√°ficos.

#### Algoritmos Implementados:

| Algoritmo | Complejidad (Promedio) | Complejidad (Peor) | Estable | Tiempo Medido |
|-----------|------------------------|-------------------|---------|---------------|
| **TimSort** | O(n log n) | O(n log n) | ‚úÖ | 0.007775 s |
| **QuickSort** | O(n log n) | O(n¬≤) | ‚ùå | 0.015041 s |
| **Gnome Sort** | O(n¬≤) | O(n¬≤) | ‚úÖ | 0.032919 s |
| **Tree Sort** | O(n log n) | O(n¬≤) | ‚úÖ | 0.082040 s |
| **Comb Sort** | O(n¬≤/2·µñ) | O(n¬≤) | ‚ùå | 0.089920 s |
| **Binary Insertion** | O(n¬≤) | O(n¬≤) | ‚úÖ | 0.092856 s |
| **Pigeonhole Sort** | O(n + N) | O(n + N) | ‚úÖ | 0.182685 s |
| **Bucket Sort** | O(n + k) | O(n¬≤) | ‚úÖ | 0.225239 s |
| **Bitonic Sort** | O(log¬≤ n) | O(log¬≤ n) | ‚ùå | 0.360274 s |
| **Heap Sort** | O(n log n) | O(n log n) | ‚ùå | 1.478462 s |
| **Radix Sort** | O(d √ó n) | O(d √ó n) | ‚úÖ | 2.360685 s |
| **Selection Sort** | O(n¬≤) | O(n¬≤) | ‚ùå | 8.256235 s |

#### Criterios de Ordenamiento:
1. **Primario**: A√±o de publicaci√≥n (ascendente)
2. **Secundario**: T√≠tulo (alfab√©tico)

#### Caracter√≠sticas:
- ‚úÖ Implementaci√≥n desde cero (sin `sorted()` nativo)
- ‚úÖ Medici√≥n precisa de tiempos de ejecuci√≥n
- ‚úÖ Manejo de campos faltantes o inv√°lidos
- ‚úÖ Normalizaci√≥n de texto (lowercase, sin acentos)
- ‚úÖ Estad√≠sticas comparativas con gr√°ficos

#### Outputs:
```
data/processed/ordenamiento/
  ‚îú‚îÄ‚îÄ ordenado_timsort.bib
  ‚îú‚îÄ‚îÄ ordenado_quicksort.bib
  ‚îú‚îÄ‚îÄ ordenado_heap.bib
  ‚îî‚îÄ‚îÄ ... (12 archivos)
Seguimiento_1/
  ‚îî‚îÄ‚îÄ comparacion_algoritmos.png  # Gr√°fico de barras
```

---

### üß¨ Requerimiento 2: Similitud Textual

**Objetivo**: Comparar abstracts de art√≠culos usando m√∫ltiples t√©cnicas NLP.

#### Algoritmos Implementados:

**Cl√°sicos (sin IA):**
- ‚úÖ **TF-IDF** con similitud coseno
- ‚úÖ **LSA** (Latent Semantic Analysis)
- ‚úÖ **LDA** (Latent Dirichlet Allocation)

**Basados en IA:**
- ‚úÖ **Sentence-BERT** (`all-MiniLM-L6-v2`)
- ‚úÖ **RoBERTa** embeddings

#### Pipeline de Procesamiento:
```
Abstract 1, Abstract 2, Abstract 3
         ‚Üì
 1. Limpieza de texto
    - Lowercase
    - Eliminaci√≥n de stopwords
    - Lematizaci√≥n
         ‚Üì
 2. Vectorizaci√≥n
    - TF-IDF Matrix
    - Embeddings neuronales
         ‚Üì
 3. C√°lculo de Similitud
    - Coseno
    - Distancia euclidiana
         ‚Üì
 4. Ranking de Pares
    - Top 10 m√°s similares
    - Top 10 menos similares
```

#### Outputs:
```
data/processed/
  ‚îú‚îÄ‚îÄ similitud_resultados.json      # Resultados completos
  ‚îú‚îÄ‚îÄ reporte_similitud.md           # Reporte Markdown
  ‚îî‚îÄ‚îÄ reporte_similitud_top.csv      # Top pares CSV
```

#### Ejemplo de Output JSON:
```json
{
  "metadata": {
    "execution_date": "2024-11-05 10:30:45",
    "input_indices": [0, 3, 7],
    "algorithms": ["tfidf", "sentence_bert"],
    "total_pairs": 3
  },
  "results": [
    {
      "pair": "0 vs 3",
      "titles": ["Article A", "Article B"],
      "tfidf_similarity": 0.75,
      "sentence_bert_similarity": 0.82,
      "avg_similarity": 0.785
    }
  ]
}
```

---

### üìà Requerimiento 3: An√°lisis de Frecuencia

**Objetivo**: Extraer t√©rminos relevantes y evaluar su representatividad.

#### Pipeline:
```
BibTeX Corpus
     ‚Üì
1. T√©rminos Semilla (ground truth)
   - "sorting algorithm"
   - "data structure"
   - "computational complexity"
   - ... (15 t√©rminos definidos manualmente)
     ‚Üì
2. C√°lculo de Frecuencias
   - Por documento: ¬øEn cu√°ntos aparece?
   - Global: Total de ocurrencias
     ‚Üì
3. Extracci√≥n Autom√°tica (TF-IDF)
   - max_features=15
   - min_df=2 (m√≠nimo 2 documentos)
   - Vectorizaci√≥n TF-IDF
     ‚Üì
4. Evaluaci√≥n de Precisi√≥n
   - Similitud sem√°ntica (embeddings)
   - threshold=0.50
   - T√©rminos auto vs semilla
     ‚Üì
5. Reporte JSON + Consola
```

#### M√©tricas:
- **Precisi√≥n sem√°ntica**: % de t√©rminos auto-generados relevantes
- **Cobertura**: T√©rminos semilla encontrados en corpus
- **Diversidad**: Varianza en distribuci√≥n de frecuencias

#### Outputs:
```
requirement_3/
  ‚îî‚îÄ‚îÄ req3_resultados.json
```

#### Ejemplo de Output:
```json
{
  "seed_terms": {
    "sorting algorithm": {
      "frequency": 45,
      "documents": 38
    }
  },
  "auto_terms": {
    "binary search tree": {
      "tfidf_score": 0.87,
      "is_relevant": true
    }
  },
  "evaluation": {
    "precision": 0.73,
    "relevant_count": 11,
    "total_auto": 15
  }
}
```

---

### üå≥ Requerimiento 4: Clustering Jer√°rquico

**Objetivo**: Agrupar documentos por similitud sem√°ntica.

#### Algoritmos:
- ‚úÖ **Agglomerative Clustering** (ward, complete, average linkage)
- ‚úÖ **DBSCAN** (para comparaci√≥n)

#### Proceso:
```
Corpus (n=25 abstracts)
     ‚Üì
1. Vectorizaci√≥n TF-IDF
     ‚Üì
2. Clustering Jer√°rquico
   - Linkage: ward, complete, average
     ‚Üì
3. Generaci√≥n de Dendrogramas
   - Visualizaci√≥n PNG
   - Etiquetas: Autor + A√±o
     ‚Üì
4. Exportaci√≥n de Grupos
```

#### Outputs:
```
data/processed/
  ‚îú‚îÄ‚îÄ dendrogram_ward.png
  ‚îú‚îÄ‚îÄ dendrogram_complete.png
  ‚îú‚îÄ‚îÄ dendrogram_average.png
  ‚îî‚îÄ‚îÄ clusters_assignment.json
```

---

### üó∫Ô∏è Requerimiento 5: Visualizaciones Avanzadas

**Objetivo**: Generar an√°lisis visual multidimensional.

#### Visualizaciones:

**1. Mapa de Calor Geogr√°fico**
- Distribuci√≥n de primer autor por pa√≠s
- Mapa mundial interactivo (Plotly)
- Gr√°fico de barras top 10 pa√≠ses

**2. Nube de Palabras**
- Abstracts + Keywords
- M√°ximo 150 palabras
- Stopwords filtradas
- Colores tem√°ticos

**3. L√≠neas Temporales**
- **Serie 1**: Publicaciones por a√±o
- **Serie 2**: Top 8 revistas por a√±o

**4. Reporte PDF Consolidado**
- Todas las visualizaciones en un PDF
- Metadatos y estad√≠sticas
- Generado con `matplotlib.backends.backend_pdf`

#### Outputs:
```
requirement_5/
  ‚îú‚îÄ‚îÄ heatmap_geo.png
  ‚îú‚îÄ‚îÄ heatmap_geo.html        # Interactivo
  ‚îú‚îÄ‚îÄ wordcloud.png
  ‚îú‚îÄ‚îÄ timeline_year.png
  ‚îú‚îÄ‚îÄ timeline_journal.png
  ‚îî‚îÄ‚îÄ reporte_completo.pdf    # Consolidado
```

---

## üéÆ Uso del Sistema

### Opci√≥n 1: Ejecutar Todo el Pipeline

```bash
# Ejecutar todos los requerimientos en secuencia
python run_all.py

# Con par√°metros personalizados
python run_all.py \
  --bib data/processed/productos_unificados.bib \
  --req2 0 5 10 \
  --req4n 30 \
  --wcmax 200 \
  --topj 10
```

### Opci√≥n 2: Ejecutar Requerimientos Individuales

#### Requerimiento 1: Scraping
```bash
python main.py req1
```

#### Seguimiento 1: Algoritmos de Ordenamiento
```bash
python Seguimiento_1/algoritmos_ordenamiento.py
```

#### Requerimiento 2: Similitud
```bash
# Comparar abstracts 0, 3 y 7
python main.py req2 0 3 7

# Comparar m√°s abstracts
python main.py req2 0 5 10 15 20
```

#### Requerimiento 3: Frecuencias
```bash
python main.py req3 --max-terms 20 --min-df 3 --thr 0.60
```

#### Requerimiento 4: Clustering
```bash
python main.py req4 --n 25
```

#### Requerimiento 5: Visualizaciones
```bash
python main.py req5 --wc-max 150 --topj 8
```

### Opci√≥n 3: Ver Ayuda

```bash
# Ayuda general
python main.py --help

# Ayuda por comando
python main.py req2 --help
python main.py req3 --help
```

---

## üåê Interfaz Web

El proyecto incluye una **interfaz web Flask** para ejecutar an√°lisis sin CLI.

### Iniciar el Servidor

```bash
python webui.py
```

Abre tu navegador en: **http://127.0.0.1:5000**

### Funcionalidades Web:

- ‚úÖ **Dashboard Interactivo**: Vista general de todos los requerimientos
- ‚úÖ **Ejecuci√≥n de Scripts**: Botones para cada requerimiento
- ‚úÖ **Visualizaci√≥n de Resultados**: Im√°genes y gr√°ficos embebidos
- ‚úÖ **Descarga de Archivos**: CSV, JSON, PDF, PNG
- ‚úÖ **Logs en Tiempo Real**: Ver progreso de ejecuci√≥n
- ‚úÖ **Responsive Design**: Compatible con m√≥viles

### Capturas de Pantalla:

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  üìä Sistema de An√°lisis Bibliom√©trico   ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îÇ
‚îÇ  ‚îÇ  Req1  ‚îÇ ‚îÇ  Req2  ‚îÇ ‚îÇ  Req3  ‚îÇ       ‚îÇ
‚îÇ  ‚îÇ [Run]  ‚îÇ ‚îÇ [Run]  ‚îÇ ‚îÇ [Run]  ‚îÇ       ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò       ‚îÇ
‚îÇ                                         ‚îÇ
‚îÇ  üìà Resultados Recientes:               ‚îÇ
‚îÇ  ‚Ä¢ productos_unificados.bib (2.1 MB)    ‚îÇ
‚îÇ  ‚Ä¢ similitud_resultados.json (145 KB)   ‚îÇ
‚îÇ  ‚Ä¢ dendrogram_ward.png (256 KB)         ‚îÇ
‚îÇ                                         ‚îÇ
‚îÇ  üì• Descargar Todos los Resultados      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üìä Metodolog√≠a Cient√≠fica

### Dise√±o Experimental

**Pregunta de Investigaci√≥n:**
> ¬øC√≥mo se comparan los algoritmos de ordenamiento cl√°sicos en datasets bibliogr√°ficos reales?

**Hip√≥tesis:**
- H1: TimSort ser√° el m√°s eficiente (O(n log n) h√≠brido)
- H2: Algoritmos cuadr√°ticos (Selection, Gnome) ser√°n los m√°s lentos
- H3: Radix Sort tendr√° buen desempe√±o te√≥rico pero overhead pr√°ctico

**Variables:**
- **Independiente**: Algoritmo de ordenamiento
- **Dependiente**: Tiempo de ejecuci√≥n (segundos)
- **Controladas**: Dataset (mismo BibTeX), hardware, Python 3.11

### Resultados Estad√≠sticos

#### An√°lisis de Tiempos:

| Categor√≠a | Algoritmo | Tiempo (s) | Speedup vs Peor |
|-----------|-----------|------------|-----------------|
| **√ìptimos** | TimSort | 0.0078 | 1059x |
| | QuickSort | 0.0150 | 550x |
| | Gnome Sort | 0.0329 | 251x |
| **Buenos** | Tree Sort | 0.0820 | 101x |
| | Comb Sort | 0.0899 | 92x |
| | Binary Insertion | 0.0929 | 89x |
| **Aceptables** | Pigeonhole | 0.1827 | 45x |
| | Bucket Sort | 0.2252 | 37x |
| | Bitonic Sort | 0.3603 | 23x |
| **Lentos** | Heap Sort | 1.4785 | 5.6x |
| | Radix Sort | 2.3607 | 3.5x |
| **Muy Lentos** | Selection Sort | 8.2562 | 1x |

#### Conclusiones:

1. **TimSort es el claro ganador** (implementaci√≥n nativa de Python)
2. **QuickSort** tiene excelente balance complejidad/velocidad
3. **Selection Sort** es el peor por su O(n¬≤) puro
4. **Radix Sort** tiene overhead de memoria y buckets
5. **Heap Sort** sufre por constantes multiplicativas grandes

---

## üì¶ Resultados y Outputs

### Directorio de Salidas

```
data/processed/
‚îú‚îÄ‚îÄ productos_unificados.bib         # Dataset unificado (1500+ entradas)
‚îú‚îÄ‚îÄ ordenamiento/                    # 12 archivos ordenados
‚îÇ   ‚îú‚îÄ‚îÄ ordenado_timsort.bib
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îú‚îÄ‚îÄ similitud_resultados.json        # Matrices de similitud
‚îú‚îÄ‚îÄ req3_resultados.json             # Frecuencias y t√©rminos
‚îú‚îÄ‚îÄ clusters_assignment.json         # Grupos jer√°rquicos
‚îî‚îÄ‚îÄ reporte_completo.pdf             # Reporte consolidado
```

### M√©tricas de Desempe√±o

#### Scraping (Req1):
- **Tiempo promedio**: 5-10 min por b√∫squeda
- **Art√≠culos extra√≠dos**: 1500+ (ACM + SAGE)
- **Tasa de √©xito**: >95%

#### Ordenamiento (Seg1):
- **Dataset**: 1522 entradas BibTeX
- **Algoritmo m√°s r√°pido**: TimSort (7.8 ms)
- **Algoritmo m√°s lento**: Selection Sort (8.26 s)

#### Similitud (Req2):
- **Modelos evaluados**: 5 (TF-IDF, LSA, LDA, SBERT, RoBERTa)
- **Tiempo por par**: ~0.5s (SBERT)
- **Precisi√≥n promedio**: 85% (validaci√≥n manual)

#### Frecuencias (Req3):
- **T√©rminos semilla**: 15
- **T√©rminos extra√≠dos**: 15 (TF-IDF)
- **Precisi√≥n sem√°ntica**: 73%

#### Clustering (Req4):
- **Abstracts analizados**: 25
- **Linkages evaluados**: 3 (ward, complete, average)
- **Tiempo de clustering**: <2s

#### Visualizaciones (Req5):
- **Pa√≠ses mapeados**: 45+
- **Palabras en nube**: 150
- **L√≠neas temporales**: 2 (a√±o, revista)
- **Tama√±o PDF**: ~2 MB

---

## üêõ Troubleshooting

### Problema 1: ChromeDriver no funciona

**S√≠ntoma:**
```
selenium.common.exceptions.SessionNotCreatedException: 
Message: session not created: This version of ChromeDriver only supports Chrome version 120
```

**Soluci√≥n:**
```bash
# Actualizar Chrome a la √∫ltima versi√≥n
# Reinstalar undetected-chromedriver
pip install --upgrade undetected-chromedriver
```

---

### Problema 2: Error de memoria (MemoryError)

**S√≠ntoma:**
```
MemoryError: Unable to allocate array with shape (10000, 10000)
```

**Soluci√≥n:**
```python
# En requirement_2/classic.py, reducir n_samples
# Cambiar de 1000 a 500
n_samples = min(500, len(corpus))
```

---

### Problema 3: Modelos de IA no descargan

**S√≠ntoma:**
```
OSError: Can't load tokenizer for 'sentence-transformers/all-MiniLM-L6-v2'
```

**Soluci√≥n:**
```bash
# Verificar conexi√≥n a internet
ping huggingface.co

# Descargar manualmente
python -c "from sentence_transformers import SentenceTransformer; SentenceTransformer('all-MiniLM-L6-v2')"
```

---

### Problema 4: Gr√°ficos no se generan

**S√≠ntoma:**
```
RuntimeError: Invalid DISPLAY variable
```

**Soluci√≥n:**
```bash
# En entornos sin GUI (servidores)
export MPLBACKEND=Agg

# O en Python
import matplotlib
matplotlib.use('Agg')
```

---

### Problema 5: Error de encoding en Windows

**S√≠ntoma:**
```
UnicodeDecodeError: 'charmap' codec can't decode byte 0x81
```

**Soluci√≥n:**
```bash
# Configurar UTF-8 globalmente (Windows 10+)
set PYTHONIOENCODING=utf-8
set PYTHONUTF8=1

# O ejecutar con:
python -X utf8 main.py req1
```

---

## ü§ù Contribuciones

### Autores

- **N√©stor Castelblancon** - [@nestorcastelblanco](https://github.com/nestorcastelblanco)
- **Sebasti√°n Agudelo** - [@sebastianagudelom](https://github.com/sebastianagudelom)
- **Juan Felipe Hurtado** - [@felipehurtadoo](https://github.com/felipehurtadoo)

### C√≥mo Contribuir

1. **Fork** el proyecto
2. Crear una **rama** para tu feature (`git checkout -b feature/amazing-feature`)
3. **Commit** tus cambios (`git commit -m 'Add amazing feature'`)
4. **Push** a la rama (`git push origin feature/amazing-feature`)
5. Abrir un **Pull Request**

### C√≥digo de Conducta

Este es un proyecto acad√©mico. Por favor:
- ‚úÖ Documenta tu c√≥digo
- ‚úÖ Escribe tests
- ‚úÖ Mant√©n PEP 8
- ‚úÖ Respeta las licencias de librer√≠as

---

## üìú Licencia

Este proyecto es de uso **acad√©mico exclusivo**.

**Restricciones:**
- ‚ùå No usar para fines comerciales
- ‚ùå No redistribuir sin permiso
- ‚úÖ Citar al usar en trabajos acad√©micos

**Cita sugerida:**
```bibtex
@software{bibliometria_aa_2024,
  author = {Castelblanco, N√©stor; Agudelo, Sebasti√°n and Hurtado, Felipe},
  title = {Sistema de An√°lisis Bibliom√©trico y Algoritmos de Ordenamiento},
  year = {2024},
  publisher = {GitHub},
  url = {https://github.com/nestorcastelblanco/Bibliometria_AA}
}
```

---

## üìû Contacto

**Repositorio**: [github.com/nestorcastelblanco/Bibliometria_AA](https://github.com/nestorcastelblanco/Bibliometria_AA)

**Issues**: [Reportar un problema](https://github.com/nestorcastelblanco/Bibliometria_AA/issues)

---

## üôè Agradecimientos

- **Python Software Foundation** - Por Python
- **Selenium Project** - Por automatizaci√≥n web
- **HuggingFace** - Por modelos pre-entrenados
- **Plotly** - Por visualizaciones interactivas
- **ACM & SAGE** - Por acceso a bases de datos acad√©micas

---

## üìö Referencias Bibliogr√°ficas

1. Knuth, D. E. (1998). *The Art of Computer Programming, Volume 3: Sorting and Searching*. Addison-Wesley.
2. Cormen, T. H., et al. (2009). *Introduction to Algorithms*. MIT Press.
3. Manning, C. D., et al. (2008). *Introduction to Information Retrieval*. Cambridge University Press.
4. Reimers, N., & Gurevych, I. (2019). *Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks*. EMNLP.

---

<div align="center">

**‚≠ê Si este proyecto te fue √∫til, considera darle una estrella en GitHub ‚≠ê**

Made with ‚ù§Ô∏è for we

</div>
