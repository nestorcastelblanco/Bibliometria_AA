# ğŸ“ Sistema de BibliometrÃ­a con Django + Playwright

## ğŸ“‹ DescripciÃ³n

Sistema completo de anÃ¡lisis bibliomÃ©trico que combina:
- **Web Scraping** con Playwright (headless, evasiÃ³n de Cloudflare)
- **API REST** con Django REST Framework
- **Pipeline de AnÃ¡lisis** (similitud, frecuencias, clustering, visualizaciones)
- **Dashboard Web** interactivo

## ğŸ—ï¸ Arquitectura

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. SCRAPING (Google Cloud VM)                              â”‚
â”‚     - acm_scraper_playwright.py                             â”‚
â”‚     - Modo headless sin interfaz grÃ¡fica                    â”‚
â”‚     - Guarda .bib en data/raw/acm/                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  2. PIPELINE DE ANÃLISIS                                     â”‚
â”‚     - run_all.py (Req2 â†’ Req5)                              â”‚
â”‚     - Genera CSVs, imÃ¡genes, PDFs                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  3. DJANGO REST API + DASHBOARD                              â”‚
â”‚     - Lee archivos .bib y CSVs (sin DB)                    â”‚
â”‚     - API REST para consultas                               â”‚
â”‚     - Dashboard web interactivo                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ InstalaciÃ³n Local

### 1. Clonar repositorio
```bash
git clone https://github.com/nestorcastelblanco/Bibliometria_AA.git
cd Bibliometria_AA
```

### 2. Crear entorno virtual
```bash
python3 -m venv .venv
source .venv/bin/activate  # Linux/Mac
# o en Windows: .venv\Scripts\activate
```

### 3. Instalar dependencias
```bash
pip install -r requirements.txt
python3 -m playwright install chromium
python3 -m playwright install-deps  # Dependencias del sistema
```

### 4. Iniciar servidor Django
```bash
python3 manage.py runserver
```

### 5. Abrir Dashboard
Abrir navegador en: **http://127.0.0.1:8000/**

## ğŸ“¡ API Endpoints

### Papers
- `GET /api/papers/` - Lista todos los papers
- `GET /api/papers/stats/` - EstadÃ­sticas generales

### AnÃ¡lisis
- `GET /api/similarity/` - Resultados de similitud textual (Req2)
- `GET /api/frequencies/` - Frecuencias y tÃ©rminos (Req3)
- `GET /api/clusters/` - Clustering jerÃ¡rquico (Req4)

### Visualizaciones
- `GET /api/visualizations/<filename>` - ImÃ¡genes y PDFs

### Triggers (Ejecutar procesos)
- `POST /api/trigger-scrape/` - Iniciar scraping
  ```json
  {
    "pages": 5,
    "headless": true
  }
  ```

- `POST /api/trigger-analysis/` - Iniciar anÃ¡lisis
  ```json
  {
    "req2": [0, 3, 7],
    "req4n": 25
  }
  ```

## ğŸ› ï¸ Management Commands

### Ejecutar Scraper
```bash
# Scraping bÃ¡sico (2 pÃ¡ginas, headless)
python3 manage.py run_scraper

# Scraping completo (5 pÃ¡ginas)
python3 manage.py run_scraper --pages 5

# Con interfaz grÃ¡fica (solo desarrollo local)
python3 manage.py run_scraper --no-headless
```

### Ejecutar AnÃ¡lisis
```bash
# AnÃ¡lisis completo con defaults
python3 manage.py run_analysis

# AnÃ¡lisis personalizado
python3 manage.py run_analysis --req2 0 3 7 --req4n 25 --wcmax 150
```

## ğŸ“‚ Estructura de Directorios

```
Bibliometria_AA/
â”œâ”€â”€ bibliometria_web/          # ConfiguraciÃ³n Django
â”‚   â”œâ”€â”€ settings.py           # Settings con rutas de datos
â”‚   â”œâ”€â”€ urls.py               # URLs principales
â”‚   â””â”€â”€ wsgi.py               # WSGI para producciÃ³n
â”‚
â”œâ”€â”€ api/                       # API REST
â”‚   â”œâ”€â”€ views.py              # ViewSets (lee archivos)
â”‚   â””â”€â”€ urls.py               # Router de endpoints
â”‚
â”œâ”€â”€ scraper_app/               # GestiÃ³n de scrapers
â”‚   â”œâ”€â”€ management/commands/
â”‚   â”‚   â”œâ”€â”€ run_scraper.py    # Command para scraping
â”‚   â”‚   â””â”€â”€ run_analysis.py   # Command para anÃ¡lisis
â”‚   â””â”€â”€ views.py              # Dashboard view
â”‚
â”œâ”€â”€ templates/                 # Templates HTML
â”‚   â””â”€â”€ dashboard.html        # Dashboard principal
â”‚
â”œâ”€â”€ data/                      # Datos (sin DB)
â”‚   â”œâ”€â”€ raw/acm/              # .bib descargados
â”‚   â””â”€â”€ processed/            # CSVs, imÃ¡genes, PDFs
â”‚
â”œâ”€â”€ requirement_1/            # Scrapers
â”‚   â””â”€â”€ scrapers/
â”‚       â””â”€â”€ acm_scraper_playwright.py
â”‚
â”œâ”€â”€ requirement_2-5/          # Pipeline de anÃ¡lisis
â”œâ”€â”€ run_all.py               # Runner completo
â””â”€â”€ requirements.txt         # Dependencias
```

## â˜ï¸ Despliegue en Google Cloud

### 1. Crear VM para Scraping
```bash
# Crear VM Debian 11
gcloud compute instances create bibliometria-scraper \
  --machine-type=e2-medium \
  --image-family=debian-11 \
  --image-project=debian-cloud \
  --boot-disk-size=20GB

# Conectar a VM
gcloud compute ssh bibliometria-scraper
```

### 2. Configurar VM
```bash
# Instalar dependencias
sudo apt update
sudo apt install -y python3-pip python3-venv git

# Clonar repo
git clone https://github.com/nestorcastelblanco/Bibliometria_AA.git
cd Bibliometria_AA

# Configurar entorno
python3 -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
python3 -m playwright install chromium
python3 -m playwright install-deps
```

### 3. Probar Scraper en Headless
```bash
python3 requirement_1/scrapers/acm_scraper_playwright.py --pages 1
```

### 4. Configurar Cron (Scraping AutomÃ¡tico)
```bash
crontab -e

# Agregar lÃ­nea (ejecutar cada domingo a las 2 AM)
0 2 * * 0 /home/user/Bibliometria_AA/.venv/bin/python3 /home/user/Bibliometria_AA/requirement_1/scrapers/acm_scraper_playwright.py --pages 5 >> /home/user/scraper.log 2>&1
```

### 5. Desplegar Django en Cloud Run (Opcional)

#### Crear Dockerfile
```dockerfile
FROM python:3.11-slim

# Instalar dependencias del sistema
RUN apt-get update && apt-get install -y \
    git \
    && rm -rf /var/lib/apt/lists/*

WORKDIR /app

# Copiar requirements
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copiar cÃ³digo
COPY . .

# Recolectar archivos estÃ¡ticos
RUN python3 manage.py collectstatic --noinput

EXPOSE 8000

# Ejecutar con gunicorn
CMD ["gunicorn", "bibliometria_web.wsgi:application", "--bind", "0.0.0.0:8000", "--workers", "2"]
```

#### Desplegar
```bash
# Build y push a Container Registry
gcloud builds submit --tag gcr.io/PROJECT_ID/bibliometria-api

# Deploy a Cloud Run
gcloud run deploy bibliometria-api \
  --image gcr.io/PROJECT_ID/bibliometria-api \
  --platform managed \
  --region us-central1 \
  --allow-unauthenticated
```

## ğŸ”‘ Ventajas de Esta Arquitectura

âœ… **Sin Base de Datos**: Usa archivos .bib y CSVs  
âœ… **SeparaciÃ³n de Componentes**: Scraper, Pipeline y API independientes  
âœ… **Escalable**: Cada componente puede escalar por separado  
âœ… **Probado en ProducciÃ³n**: Misma stack que funcionÃ³ en Google Cloud  
âœ… **Headless**: Scraper funciona sin interfaz grÃ¡fica  

## ğŸ“Š Pipeline de AnÃ¡lisis

El pipeline completo ejecuta:

1. **Req2**: Similitud textual entre abstracts
2. **Req3**: Frecuencias y tÃ©rminos asociados
3. **Req4**: Clustering jerÃ¡rquico + dendrogramas
4. **Req5**: Heatmaps, wordclouds, timelines + PDF

### Ejecutar Pipeline Completo
```bash
python3 run_all.py --req2 0 3 7 --req4n 25 --wcmax 150
```

## ğŸ§ª Testing

### Probar API
```bash
# Stats de papers
curl http://127.0.0.1:8000/api/papers/stats/

# Trigger scraping
curl -X POST http://127.0.0.1:8000/api/trigger-scrape/ \
  -H "Content-Type: application/json" \
  -d '{"pages": 2, "headless": true}'

# Trigger anÃ¡lisis
curl -X POST http://127.0.0.1:8000/api/trigger-analysis/ \
  -H "Content-Type: application/json" \
  -d '{"req2": [0,3,7], "req4n": 25}'
```

## ğŸ“ Notas Importantes

- **NO usar en producciÃ³n** con `DEBUG=True`
- **Configurar SECRET_KEY** en producciÃ³n
- **Usar HTTPS** en producciÃ³n
- **Configurar ALLOWED_HOSTS** correctamente
- **Limitar rate de scraping** para no ser bloqueado

## ğŸ¤ CrÃ©ditos

Basado en la soluciÃ³n exitosa de Juan David GuzmÃ¡n:
- Django como framework
- Google Cloud VM (Debian sin GUI)
- Playwright en modo headless

## ğŸ“„ Licencia

MIT License
