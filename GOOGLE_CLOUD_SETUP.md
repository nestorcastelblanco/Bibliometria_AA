# üöÄ GU√çA PASO A PASO: Google Cloud Setup

## üìã CHECKLIST DE SETUP

### FASE 1: Crear Cuenta Google Cloud (10 minutos)
- [ ] Ir a https://cloud.google.com/free
- [ ] Crear cuenta con tu Gmail
- [ ] Verificar con tarjeta de cr√©dito (no te cobrar√°n)
- [ ] Activar cr√©dito de $300 USD gratis
- [ ] Aceptar t√©rminos y condiciones

### FASE 2: Crear VM para Scraping (15 minutos)
- [ ] Abrir Google Cloud Console
- [ ] Ir a "Compute Engine" ‚Üí "VM instances"
- [ ] Crear nueva instancia
- [ ] Configurar seg√∫n especificaciones
- [ ] Iniciar VM

### FASE 3: Configurar VM (30 minutos)
- [ ] Conectar por SSH
- [ ] Instalar Python y dependencias
- [ ] Instalar Playwright
- [ ] Clonar repositorio
- [ ] Configurar entorno virtual

### FASE 4: Probar Scraper (10 minutos)
- [ ] Ejecutar scraper de prueba
- [ ] Verificar que no hay bloqueo de Cloudflare
- [ ] Revisar archivos .bib descargados

### FASE 5: Automatizar con Cron (10 minutos)
- [ ] Configurar cron job
- [ ] Probar ejecuci√≥n autom√°tica
- [ ] Configurar notificaciones (opcional)

---

## üéØ FASE 1: CREAR CUENTA GOOGLE CLOUD

### Paso 1.1: Registrarse

1. **Abre tu navegador** y ve a:
   ```
   https://cloud.google.com/free
   ```

2. **Haz clic en "Empezar gratis"** o "Get started for free"

3. **Inicia sesi√≥n** con tu cuenta de Gmail

4. **Completa el formulario:**
   - Pa√≠s: Colombia (o tu pa√≠s)
   - Tipo de cuenta: Individual
   - T√©rminos: Acepta los t√©rminos

5. **Verificaci√≥n de pago:**
   - Agrega una tarjeta de cr√©dito o d√©bito
   - **IMPORTANTE:** No te cobrar√°n autom√°ticamente
   - Solo es para verificar tu identidad
   - Recibes $300 USD de cr√©dito gratis

6. **¬°Listo!** Ahora tienes acceso a Google Cloud Console

---

## üñ•Ô∏è FASE 2: CREAR VM PARA SCRAPING

### Paso 2.1: Acceder a Compute Engine

1. En Google Cloud Console, busca "Compute Engine" en el buscador superior
2. Haz clic en **"VM instances"**
3. Si es tu primera vez, espera a que se inicialice (1-2 minutos)

### Paso 2.2: Crear VM

Haz clic en **"CREATE INSTANCE"** y configura:

#### **Configuraci√≥n B√°sica:**
```
Name: bibliometria-scraper
Region: us-central1 (Iowa) - M√°s barato
Zone: us-central1-a
```

#### **Machine Configuration:**
```
Series: E2
Machine type: e2-medium
  - 2 vCPU
  - 4 GB memory
  - Costo: ~$24/mes (con cr√©dito gratis no pagas)
```

#### **Boot Disk:**
Haz clic en "CHANGE" y selecciona:
```
Operating System: Debian
Version: Debian GNU/Linux 11 (bullseye)
Boot disk type: Standard persistent disk
Size: 20 GB
```

#### **Firewall:**
```
‚òëÔ∏è Allow HTTP traffic
‚òëÔ∏è Allow HTTPS traffic
```

### Paso 2.3: Crear la VM

1. Haz clic en **"CREATE"** (abajo)
2. Espera 1-2 minutos a que se cree
3. Ver√°s tu VM en la lista con un ‚úÖ verde

---

## üîß FASE 3: CONFIGURAR VM

### Paso 3.1: Conectar por SSH

En la lista de VMs, haz clic en **"SSH"** al lado de tu VM.

Se abrir√° una terminal en tu navegador. ¬°Ya est√°s dentro de la VM!

### Paso 3.2: Instalar Dependencias del Sistema

Copia y pega estos comandos uno por uno:

```bash
# Actualizar sistema
sudo apt update && sudo apt upgrade -y

# Instalar herramientas b√°sicas
sudo apt install -y git python3 python3-pip python3-venv curl wget

# Verificar instalaci√≥n
python3 --version  # Debe mostrar Python 3.9+
git --version
```

### Paso 3.3: Clonar Repositorio

```bash
# Ir a directorio home
cd ~

# Clonar tu repo
git clone https://github.com/nestorcastelblanco/Bibliometria_AA.git

# Entrar al directorio
cd Bibliometria_AA

# Ver archivos
ls -la
```

### Paso 3.4: Crear Entorno Virtual

```bash
# Crear entorno virtual
python3 -m venv .venv

# Activar entorno virtual
source .venv/bin/activate

# Deber√≠as ver (.venv) al inicio de tu prompt
```

### Paso 3.5: Instalar Dependencias Python

```bash
# Actualizar pip
pip install --upgrade pip

# Instalar dependencias
pip install -r requirements.txt

# Esto tardar√° 2-3 minutos
```

### Paso 3.6: Instalar Playwright

```bash
# Instalar Playwright
python3 -m playwright install chromium

# Instalar dependencias del sistema para Playwright
python3 -m playwright install-deps

# Esto tardar√° 3-5 minutos y pedir√° contrase√±a
# La contrase√±a es tu password de Google (el que usas en Gmail)
```

---

## üß™ FASE 4: PROBAR SCRAPER

### Paso 4.1: Probar Scraper (1 p√°gina)

```bash
# Aseg√∫rate de estar en el directorio correcto
cd ~/Bibliometria_AA

# Activar entorno virtual si no est√° activo
source .venv/bin/activate

# Ejecutar scraper de prueba
python3 requirement_1/scrapers/acm_scraper_playwright.py --pages 1
```

**¬°ESTE ES EL MOMENTO CR√çTICO!**

Si ves:
```
‚úÖ P√°gina cargada: Search Results ‚Äì ACM Digital Library
‚òëÔ∏è Seleccionando resultados...
‚úÖ Resultados seleccionados: 50
```

**¬°√âXITO! üéâ** Cloudflare no te est√° bloqueando.

Si ves:
```
‚ö†Ô∏è Cloudflare detectado: Just a moment...
```

Espera 20 segundos y prueba de nuevo. Las IPs de Google Cloud son m√°s confiables.

### Paso 4.2: Verificar Archivos Descargados

```bash
# Ver archivos descargados
ls -lh ~/Bibliometria_AA/data/raw/acm/

# Contar archivos .bib
ls ~/Bibliometria_AA/data/raw/acm/*.bib | wc -l

# Ver contenido del √∫ltimo archivo
ls -t ~/Bibliometria_AA/data/raw/acm/*.bib | head -1 | xargs head -20
```

### Paso 4.3: Probar Scraping Completo (5 p√°ginas)

Si el test funcion√≥, prueba con m√°s p√°ginas:

```bash
python3 requirement_1/scrapers/acm_scraper_playwright.py --pages 5
```

Esto tardar√° 5-10 minutos.

---

## ‚è∞ FASE 5: AUTOMATIZAR CON CRON

### Paso 5.1: Crear Script de Scraping

```bash
# Crear script ejecutable
nano ~/scrape_acm.sh
```

Pega este contenido:

```bash
#!/bin/bash
# Script para ejecutar scraping autom√°tico

# Activar entorno virtual
source /home/$(whoami)/Bibliometria_AA/.venv/bin/activate

# Ir al directorio del proyecto
cd /home/$(whoami)/Bibliometria_AA

# Ejecutar scraper
python3 requirement_1/scrapers/acm_scraper_playwright.py --pages 5 >> /home/$(whoami)/scraper.log 2>&1

# Sincronizar archivos (si usas Cloud Storage)
# gsutil rsync -r data/raw/acm/ gs://tu-bucket/acm/

echo "Scraping completado: $(date)" >> /home/$(whoami)/scraper.log
```

Guarda con `Ctrl+O`, `Enter`, `Ctrl+X`

```bash
# Dar permisos de ejecuci√≥n
chmod +x ~/scrape_acm.sh

# Probar script
~/scrape_acm.sh
```

### Paso 5.2: Configurar Cron Job

```bash
# Abrir crontab
crontab -e

# Si pregunta por editor, selecciona nano (opci√≥n 1)
```

Agrega esta l√≠nea al final:

```bash
# Ejecutar cada domingo a las 2 AM
0 2 * * 0 /home/$(whoami)/scrape_acm.sh
```

**Otras opciones de frecuencia:**
```bash
# Cada d√≠a a las 3 AM
0 3 * * * /home/$(whoami)/scrape_acm.sh

# Cada lunes a las 8 AM
0 8 * * 1 /home/$(whoami)/scrape_acm.sh

# Cada 12 horas
0 */12 * * * /home/$(whoami)/scrape_acm.sh
```

Guarda con `Ctrl+O`, `Enter`, `Ctrl+X`

### Paso 5.3: Verificar Cron

```bash
# Ver cron jobs configurados
crontab -l

# Ver logs de cron
sudo tail -f /var/log/syslog | grep CRON
```

---

## üìä MONITOREO Y MANTENIMIENTO

### Ver Logs del Scraper

```bash
# Ver √∫ltimas 50 l√≠neas
tail -50 ~/scraper.log

# Ver en tiempo real
tail -f ~/scraper.log

# Buscar errores
grep -i error ~/scraper.log
```

### Revisar Archivos Descargados

```bash
# Contar papers
ls ~/Bibliometria_AA/data/raw/acm/*.bib | wc -l

# Ver √∫ltimo archivo
ls -t ~/Bibliometria_AA/data/raw/acm/*.bib | head -1 | xargs ls -lh

# Espacio usado
du -sh ~/Bibliometria_AA/data/
```

### Descargar Archivos a tu Mac

Desde tu terminal local (Mac):

```bash
# Usando gcloud
gcloud compute scp \
  bibliometria-scraper:~/Bibliometria_AA/data/raw/acm/*.bib \
  ~/Documents/GitHub/Bibliometria_AA/data/raw/acm/ \
  --zone=us-central1-a

# O descargar directamente desde la consola SSH
# Haz clic derecho en los archivos y "Download"
```

---

## üí∞ COSTOS ESTIMADOS

### VM e2-medium (2 vCPU, 4 GB RAM)
```
Costo mensual: ~$24 USD
Con cr√©dito de $300: GRATIS por ~12 meses
Despu√©s del cr√©dito: $24/mes
```

### Storage (20 GB disco)
```
Costo mensual: ~$0.80 USD
Con cr√©dito: GRATIS
```

### Egress (descarga de datos)
```
Primer GB al mes: GRATIS
Despu√©s: $0.12/GB
```

**Total estimado:** $25/mes despu√©s del cr√©dito de $300

### C√≥mo Ahorrar:
- Apagar VM cuando no la uses: `$0`
- Usar VM e2-micro (0.25 GB): `$7/mes`
- Ejecutar solo 1 vez por semana: Mismo costo

---

## üö® TROUBLESHOOTING

### Error: "Permission denied"
```bash
sudo chmod +x ~/scrape_acm.sh
```

### Error: "playwright not found"
```bash
source .venv/bin/activate
python3 -m playwright install chromium
```

### Cloudflare sigue bloqueando
```bash
# Cambiar regi√≥n de la VM
# Crear nueva VM en europe-west1 o asia-southeast1
# IPs de diferentes regiones tienen diferentes reputaciones
```

### VM muy lenta
```bash
# Upgrade a e2-standard-2 (2 vCPU, 8 GB)
# Costo: ~$50/mes
```

---

## ‚úÖ CHECKLIST FINAL

Antes de terminar, verifica:

- [ ] VM est√° corriendo (luz verde en Console)
- [ ] SSH funciona
- [ ] Python y Playwright instalados
- [ ] Scraper de prueba funcion√≥ (1 p√°gina)
- [ ] Archivos .bib se crearon
- [ ] Cron job configurado
- [ ] Script de backup funciona

---

## üìû SOPORTE

Si algo falla:
1. Revisa logs: `cat ~/scraper.log`
2. Prueba manualmente: `python3 requirement_1/scrapers/acm_scraper_playwright.py --pages 1`
3. Verifica Playwright: `python3 -m playwright --version`
4. Revisa el archivo `CLOUDFLARE_PROBLEMA.md`

---

**¬°Empecemos! Dime cuando est√©s listo y te ayudo con cada paso.**
